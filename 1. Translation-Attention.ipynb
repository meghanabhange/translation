{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bnQuVhpaKIS",
    "outputId": "7a589e9c-9d0a-4c95-da84-506e32f66d95"
   },
   "source": [
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IPjQJKGbbCl-",
    "outputId": "510ea6dc-1b98-415c-b3ed-9a97f4576401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 20.0MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20kB 24.8MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30kB 27.3MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40kB 24.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 51kB 15.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 61kB 13.3MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71kB 14.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 81kB 14.9MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 92kB 13.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 102kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 112kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 122kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 133kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 143kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 153kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 163kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 174kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 184kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 194kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 204kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 215kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 225kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 235kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 245kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 256kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 266kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 276kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 286kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 296kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 307kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 317kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 327kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 337kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 348kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 358kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 368kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 378kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 389kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 399kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 409kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 419kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 430kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 440kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 450kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 460kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 471kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 481kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 491kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 501kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 512kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 522kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 532kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 542kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 552kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 563kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 573kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 583kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 593kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 604kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 614kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 624kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 634kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 645kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 655kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 665kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 675kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 686kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 696kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 706kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 716kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 727kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 737kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 747kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 757kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 768kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 778kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 788kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 798kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 808kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 819kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 829kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 839kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 849kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 860kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 870kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 880kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 890kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 901kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 911kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 921kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 931kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 942kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 952kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 962kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 972kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 983kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 993kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 1.0MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.0MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.0MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.0MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.0MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 1.1MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.1MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 1.1MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.1MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.1MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.1MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 1.1MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.1MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 1.1MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.2MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.2MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.2MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 1.2MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.2MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.2MB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.2MB 14.5MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4t-Me0pIi8ek"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AiMJM2Rn9vaL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.legacy.data import BucketIterator, Field\n",
    "from torchtext.legacy.datasets import Multi30k, TranslationDataset\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from translate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zgyiXzUl9vaN"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7m8oeAF8rrl"
   },
   "source": [
    "# En-De Translation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zk-RaF6xywUX",
    "outputId": "6b268da2-52e0-47ff-dca2-a74a83ffd4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  3  615M    3 20.8M    0     0  6388k      0  0:01:38  0:00:03  0:01:35 6386k"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!curl https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en --output data/train.en\n",
    "!curl https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de --output data/train.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQ7lkMAY0uaj"
   },
   "outputs": [],
   "source": [
    "def restructure_data(\n",
    "    train_en: Path,\n",
    "    train_de: Path,\n",
    "    train_data_output: Path,\n",
    "    validation_data_output,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Converts the text data into a json of list of dictionaries which map\n",
    "    English sentence to corresponding German Sentence\n",
    "\n",
    "    args :\n",
    "      train_en : [ Path ] : Training data path for English Sentences\n",
    "      train_de : [ Path ] : Training data path for German Sentences\n",
    "      train_data_output : [ Path ] : Training data output path\n",
    "      validation_data_output : [ Path ] : Validation data path\n",
    "      test_size : [ float ] : size of test split (OPTIONAL) DEFAULT=0.33\n",
    "      random_state : [ float ] : random state of train-test-split (OPTIONAL) DEFAULT=42\n",
    "\n",
    "    Returns :\n",
    "        [dict]: Training Data\n",
    "        [dict]: Validation data\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        {\"src\": en, \"trg\": de}\n",
    "        for en, de in zip(train_en.open().readlines(), train_de.open().readlines())\n",
    "    ]\n",
    "    train_data, dev_data = train_test_split(\n",
    "        data, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    json.dump(train_data, train_data_output.open(\"w\"), indent=2)\n",
    "    json.dump(dev_data, validation_data_output.open(\"w\"), indent=2)\n",
    "    return train_data, dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzpaQnIshdgR"
   },
   "outputs": [],
   "source": [
    "def split_train_test(train_path, output_dir, lang, test_size=0.33, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits train data in train, dev and test\n",
    "\n",
    "    args:\n",
    "      train_path : [ Path ] : Training data path for lang\n",
    "      output_dir : [ Path/string ] : Output directory for the splits\n",
    "      lang : [ str ] : Language of the training file\n",
    "      test_size : [ float ] : size of test split (OPTIONAL) DEFAULT=0.33\n",
    "      random_state : [ float ] : random state of train-test-split (OPTIONAL) DEFAULT=42\n",
    "    \"\"\"\n",
    "    output_dir = str(output_dir)\n",
    "    data = train_path.open().readlines()\n",
    "    train_data, test_data = train_test_split(\n",
    "        data, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    train_data, dev_data = train_test_split(\n",
    "        train_data, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    Path(f\"{output_dir}/train_sample.{lang}\").open(\"w\").write(\"\\n\".join(train_data))\n",
    "    Path(f\"{output_dir}/test_sample.{lang}\").open(\"w\").write(\"\\n\".join(test_data))\n",
    "    Path(f\"{output_dir}/dev_sample.{lang}\").open(\"w\").write(\"\\n\".join(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfITQcuP12NS"
   },
   "outputs": [],
   "source": [
    "data_path = Path(\"data\")\n",
    "train_en = data_path / \"train.en\"\n",
    "train_de = data_path / \"train.de\"\n",
    "train_data_output = data_path / \"train.json\"\n",
    "validation_data_output = data_path / \"dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiC6wvUV2tt2"
   },
   "outputs": [],
   "source": [
    "# train_data, dev_data = restructure_data(train_en, train_de, train_data_output, validation_data_output)\n",
    "split_train_test(train_en, \"data\", \"en\", test_size=0.33, random_state=SEED)\n",
    "split_train_test(train_de, \"data\", \"de\", test_size=0.33, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgARvm3IjE19"
   },
   "source": [
    "# SentencePiece Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwSK9JfGjG2F"
   },
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "\n",
    "\n",
    "def train_sp(data_path, out_path):\n",
    "    \"\"\"\n",
    "    Training SentencePiece Tokenize\n",
    "    args:\n",
    "      data_path : [ Path/str ] : Path for text data for training\n",
    "      out_path : [ Path/str ] : Path for output of the model with the name\n",
    "    \"\"\"\n",
    "    data_path = str(data_path)\n",
    "    out_path = str(out_path)\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        f\"--input={data_path} --model_prefix={out_path} --vocab_size=32000\"\n",
    "    )\n",
    "\n",
    "\n",
    "train_sp(\"data/train_sample.en\", \"en\")\n",
    "train_sp(\"data/train_sample.de\", \"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUEG2j1W9AZP"
   },
   "source": [
    "# Tokenisation\n",
    "\n",
    "SentencePiece Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXcQSjgGdIqm"
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    sp.load(\"de.model\")\n",
    "    return sp.encode_as_pieces(text)\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    sp.load(\"en.model\")\n",
    "    return sp.encode_as_pieces(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g0g-HPq9vaP"
   },
   "outputs": [],
   "source": [
    "SRC = Field(\n",
    "    tokenize=tokenize_de,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    truncate_first=True,\n",
    "    fix_length=64,\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    tokenize=tokenize_en,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    fix_length=64,\n",
    "    lower=True,\n",
    "    truncate_first=True,\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_KnCJZL9vaR"
   },
   "outputs": [],
   "source": [
    "train_data = TranslationDataset(\n",
    "    path=\"data/train_sample\", exts=(\".en\", \".de\"), fields=(SRC, TRG)\n",
    ")\n",
    "\n",
    "valid_data = TranslationDataset(\n",
    "    path=\"data/dev_sample\", exts=(\".en\", \".de\"), fields=(SRC, TRG)\n",
    ")\n",
    "\n",
    "test_data = TranslationDataset(\n",
    "    path=\"data/test_sample\", exts=(\".en\", \".de\"), fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eWD2w0X9vaS"
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baTtXB_M9vaT"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U01CsZBG9vaT"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coU8pyOd9vae"
   },
   "source": [
    "## Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYSAUS6U9vae"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.01\n",
    "DEC_DROPOUT = 0.01\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apeX53d49vaf"
   },
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SSg6y5Q9vag"
   },
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLQZMMH29vag"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppQdFGEb9vah"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQFOswgJ9vaj"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'ende-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dz-N0Z6j9vak"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('ende-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eSEhV0-9vak"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now we can can translations from our model with the `translate_sentence` function below.\n",
    "\n",
    "The steps taken are:\n",
    "- tokenize the source sentence if it has not been tokenized (is a string)\n",
    "- append the `<sos>` and `<eos>` tokens\n",
    "- numericalize the source sentence\n",
    "- convert it to a tensor and add a batch dimension\n",
    "- create the source sentence mask\n",
    "- feed the source sentence and mask into the encoder\n",
    "- create a list to hold the output sentence, initialized with an `<sos>` token\n",
    "- while we have not hit a maximum length\n",
    "  - convert the current output sentence prediction into a tensor with a batch dimension\n",
    "  - create a target sentence mask\n",
    "  - place the current output, encoder output and both masks into the decoder\n",
    "  - get next output token prediction from decoder along with attention\n",
    "  - add prediction to current output sentence prediction\n",
    "  - break if the prediction was an `<eos>` token\n",
    "- convert the output sentence from indexes to tokens\n",
    "- return the output sentence (with the `<sos>` token removed) and the attention from the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3W4eNe99val"
   },
   "outputs": [],
   "source": [
    "example_idx = 8\n",
    "\n",
    "src = vars(train_data.examples[example_idx])[\"src\"]\n",
    "trg = vars(train_data.examples[example_idx])[\"trg\"]\n",
    "\n",
    "print(f\"src = {src}\")\n",
    "print(f\"trg = {trg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3qRP84L9vam"
   },
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iFFXtgs9vao"
   },
   "outputs": [],
   "source": [
    "example_idx = 6\n",
    "\n",
    "src = vars(valid_data.examples[example_idx])['src']\n",
    "trg = vars(valid_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbzT-67x9vaq"
   },
   "source": [
    "## BLEU\n",
    "\n",
    "Finally we calculate the BLEU score for the Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9ZyOYBM9vaq"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        \n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        \n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        \n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMr8gCb89vaq"
   },
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy_of_6_Attention_is_All_You_Need.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
