{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bnQuVhpaKIS",
    "outputId": "7a589e9c-9d0a-4c95-da84-506e32f66d95"
   },
   "source": [
    "# Sequence To Sequence Using Attention\n",
    "\n",
    "----\n",
    "\n",
    "Goals : \n",
    "\n",
    "- Create a sequence to sequence model using transformers based on Attention Is All You Need. \n",
    "- Dataset :\n",
    "    - The dataset used here is from Stanford Neural Machine Translation. Data Link\n",
    "    - The data is split between train, dev and test using split_train_test\n",
    "- Evaluation :\n",
    "    - Training and Validation is evaluated on perplexity\n",
    "    - Additional Metrics are evaluated in check_metrics, where we want to check the following scores for generated text\n",
    "        - Levenshtein Distance\n",
    "        - BLEU Score\n",
    "        - BERT Score\n",
    "        - Rougue \n",
    "        - METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IPjQJKGbbCl-",
    "outputId": "510ea6dc-1b98-415c-b3ed-9a97f4576401"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t-Me0pIi8ek"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiMJM2Rn9vaL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.legacy.data import BucketIterator, Field\n",
    "from torchtext.legacy.datasets import Multi30k, TranslationDataset\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from translate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgyiXzUl9vaN"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7m8oeAF8rrl"
   },
   "source": [
    "# En-De Translation Data\n",
    "\n",
    "---\n",
    "\n",
    "The training data two splits for train and test. For evaluation we will split this data into train, test and dev. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zk-RaF6xywUX",
    "outputId": "6b268da2-52e0-47ff-dca2-a74a83ffd4f6"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!curl https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en --output data/train.en\n",
    "!curl https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de --output data/train.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzpaQnIshdgR"
   },
   "outputs": [],
   "source": [
    "def split_train_test(train_path, output_dir, lang, test_size=0.33, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits train data in train, dev and test\n",
    "\n",
    "    args:\n",
    "      train_path : [ Path ] : Training data path for lang\n",
    "      output_dir : [ Path/string ] : Output directory for the splits\n",
    "      lang : [ str ] : Language of the training file\n",
    "      test_size : [ float ] : size of test split (OPTIONAL) DEFAULT=0.33\n",
    "      random_state : [ float ] : random state of train-test-split (OPTIONAL) DEFAULT=42\n",
    "    \"\"\"\n",
    "    output_dir = str(output_dir)\n",
    "    data = train_path.open().readlines()\n",
    "    train_data, test_data = train_test_split(\n",
    "        data, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    train_data, dev_data = train_test_split(\n",
    "        train_data, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    Path(f\"{output_dir}/train_sample.{lang}\").open(\"w\").write(\"\\n\".join(train_data))\n",
    "    Path(f\"{output_dir}/test_sample.{lang}\").open(\"w\").write(\"\\n\".join(test_data))\n",
    "    Path(f\"{output_dir}/dev_sample.{lang}\").open(\"w\").write(\"\\n\".join(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfITQcuP12NS"
   },
   "outputs": [],
   "source": [
    "data_path = Path(\"data\")\n",
    "train_en = data_path / \"train.en\"\n",
    "train_de = data_path / \"train.de\"\n",
    "train_data_output = data_path / \"train.json\"\n",
    "validation_data_output = data_path / \"dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiC6wvUV2tt2"
   },
   "outputs": [],
   "source": [
    "split_train_test(train_en, \"data\", \"en\", test_size=0.33, random_state=SEED)\n",
    "split_train_test(train_de, \"data\", \"de\", test_size=0.33, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgARvm3IjE19"
   },
   "source": [
    "# SentencePiece Training\n",
    "\n",
    "---\n",
    "\n",
    "[Sentencepeice](https://github.com/google/sentencepiece) is an unsupervised tokenizer which takes into consideration the subwords from the training document and can be used in place of a naive text.split() token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwSK9JfGjG2F"
   },
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "\n",
    "\n",
    "def train_sp(data_path, out_path):\n",
    "    \"\"\"\n",
    "    Training SentencePiece Tokenize\n",
    "    args:\n",
    "      data_path : [ Path/str ] : Path for text data for training\n",
    "      out_path : [ Path/str ] : Path for output of the model with the name\n",
    "    \"\"\"\n",
    "    data_path = str(data_path)\n",
    "    out_path = str(out_path)\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        f\"--input={data_path} --model_prefix={out_path} --vocab_size=32000\"\n",
    "    )\n",
    "\n",
    "\n",
    "train_sp(\"data/train_sample.en\", \"en\")\n",
    "train_sp(\"data/train_sample.de\", \"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUEG2j1W9AZP"
   },
   "source": [
    "# Tokenisation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXcQSjgGdIqm"
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    sp.load(\"de.model\")\n",
    "    return sp.encode_as_pieces(text)\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    sp.load(\"en.model\")\n",
    "    return sp.encode_as_pieces(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g0g-HPq9vaP"
   },
   "outputs": [],
   "source": [
    "SRC = Field(\n",
    "    tokenize=tokenize_de,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    truncate_first=True,\n",
    "    fix_length=64,\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    tokenize=tokenize_en,\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    fix_length=64,\n",
    "    lower=True,\n",
    "    truncate_first=True,\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_KnCJZL9vaR"
   },
   "outputs": [],
   "source": [
    "train_data = TranslationDataset(\n",
    "    path=\"data/train_sample\", exts=(\".en\", \".de\"), fields=(SRC, TRG)\n",
    ")\n",
    "\n",
    "valid_data = TranslationDataset(\n",
    "    path=\"data/dev_sample\", exts=(\".en\", \".de\"), fields=(SRC, TRG)\n",
    ")\n",
    "\n",
    "test_data = TranslationDataset(\n",
    "    path=\"data/test_sample\", exts=(\".en\", \".de\"), fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eWD2w0X9vaS"
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baTtXB_M9vaT"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U01CsZBG9vaT"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coU8pyOd9vae"
   },
   "source": [
    "## Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYSAUS6U9vae"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.01\n",
    "DEC_DROPOUT = 0.01\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apeX53d49vaf"
   },
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SSg6y5Q9vag"
   },
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLQZMMH29vag"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppQdFGEb9vah"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQFOswgJ9vaj"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'ende-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dz-N0Z6j9vak"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('ende-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eSEhV0-9vak"
   },
   "source": [
    "## Inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3W4eNe99val"
   },
   "outputs": [],
   "source": [
    "example_idx = 8\n",
    "\n",
    "src = vars(train_data.examples[example_idx])[\"src\"]\n",
    "trg = vars(train_data.examples[example_idx])[\"trg\"]\n",
    "\n",
    "print(f\"src = {src}\")\n",
    "print(f\"trg = {trg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3qRP84L9vam"
   },
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9iFFXtgs9vao"
   },
   "outputs": [],
   "source": [
    "example_idx = 6\n",
    "\n",
    "src = vars(valid_data.examples[example_idx])['src']\n",
    "trg = vars(valid_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbzT-67x9vaq"
   },
   "source": [
    "## Metrics \n",
    "\n",
    "- BLEU \n",
    "- WER - Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9ZyOYBM9vaq"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "from seq2seq.metrics import wer_score \n",
    "\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return bleu_score(pred_trgs, trgs)\n",
    "\n",
    "def calculate_wer(data, src_field, trg_field, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for datum in data:\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)\n",
    "        pred_trg = pred_trg[:-1]\n",
    "        \n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "    return wer_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMr8gCb89vaq"
   },
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'WER score = {wer*100:.2f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy_of_6_Attention_is_All_You_Need.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
